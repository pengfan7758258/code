{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "b站：https://www.bilibili.com/video/BV1bt421578n?spm_id_from=333.788.videopod.sections&vd_source=e4234f5ddbe45b813cf4296e06e14b9b&p=3\n",
    "DeepLearning.AI课程: https://learn.deeplearning.ai/courses/efficiently-serving-llms/lesson/etx7v/lorax\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfan/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  464,  2068,  7586, 21831, 11687,   625,   262]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      " fence and ran to the other side of the fence\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The quick brown fox jumped over the\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad(): # 不计算梯度\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "\n",
    "\n",
    "def generate(inputs, max_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_id, past_key_values = \\\n",
    "        generate_token_with_past(next_inputs)\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_id.reshape((1, 1)), # 只传入下一个token的id\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "                dim=1\n",
    "            ),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens)\n",
    "\n",
    "\n",
    "tokens = generate(inputs, max_tokens=10)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The quick brown fox jumped over the\",\n",
    "    \"The rain in Spain falls\",\n",
    "    \"What comes up must\",\n",
    "]\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(inputs, max_tokens):\n",
    "    # 给定bs大小的空列表，存储输出\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = \\\n",
    "            generate_batch_tokens_with_past(next_inputs)\n",
    "        # print(\"next_position_ids:\", next_inputs[\"position_ids\"])\n",
    "        # print(\"position_ids:\", next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1)\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)), # 只传入下一个token的id\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1, # 只传入下一个token的位置\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"],\n",
    "                torch.ones((next_token_ids.shape[0], 1)),\n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return [\"\".join(tokens) for tokens in generated_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = generate_batch(inputs, max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumped over the \u001b[31m fence and ran to the other side of the fence\u001b[0m\n",
      "\n",
      "The rain in Spain falls \u001b[31m on the first day of the month, and the\u001b[0m\n",
      "\n",
      "What comes up must \u001b[31m be a good idea.\n",
      "\n",
      "\"I think\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, generated in zip(prompts, generated_tokens):\n",
    "    print(prompt, f\"\\x1b[31m{generated}\\x1b[0m\\n\") # 红色字体显示生成的文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 吞吐量与延迟\n",
    "- 探索批处理对延迟的影响（生成每个令牌需要多长时间）。 \n",
    "- 观察吞吐量和延迟之间存在的基本权衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs= 1\n",
      "duration 0.6177549362182617\n",
      "throughput 16.18764887775313\n",
      "avg latency 0.061775493621826175\n",
      "\n",
      "bs= 2\n",
      "duration 0.9665660858154297\n",
      "throughput 20.691808137595977\n",
      "avg latency 0.09665660858154297\n",
      "\n",
      "bs= 4\n",
      "duration 0.851855993270874\n",
      "throughput 46.95629345332405\n",
      "avg latency 0.0851855993270874\n",
      "\n",
      "bs= 8\n",
      "duration 1.0085225105285645\n",
      "throughput 79.32396070968429\n",
      "avg latency 0.10085225105285645\n",
      "\n",
      "bs= 16\n",
      "duration 1.458956241607666\n",
      "throughput 109.66744268059156\n",
      "avg latency 0.1458956241607666\n",
      "\n",
      "bs= 32\n",
      "duration 1.9382057189941406\n",
      "throughput 165.10115353806125\n",
      "avg latency 0.19382057189941407\n",
      "\n",
      "bs= 64\n",
      "duration 3.983337640762329\n",
      "throughput 160.66928232514007\n",
      "avg latency 0.3983337640762329\n",
      "\n",
      "bs= 128\n",
      "duration 7.6774022579193115\n",
      "throughput 166.72306035282548\n",
      "avg latency 0.7677402257919311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "max_tokens = 10\n",
    "\n",
    "# observations\n",
    "durations = []\n",
    "throughputs = []\n",
    "latencies = []\n",
    "\n",
    "batch_sizes = [2**p for p in range(8)] # [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"bs= {batch_size}\")\n",
    "\n",
    "    # 记录不同batch size下的生成时间和吞吐量\n",
    "    t0 = time.time()\n",
    "    batch_prompts = [\n",
    "        prompts[i % len(prompts)] for i in range(batch_size) # 重复使用prompts\n",
    "    ]\n",
    "    inputs = tokenizer(\n",
    "        batch_prompts, padding=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    generated_tokens = generate_batch(inputs, max_tokens=max_tokens)\n",
    "    duration_s = time.time() - t0\n",
    "\n",
    "    ntokens = batch_size * max_tokens\n",
    "    throughput = ntokens / duration_s\n",
    "    avg_latency = duration_s / max_tokens\n",
    "    print(\"duration\", duration_s)\n",
    "    print(\"throughput\", throughput)\n",
    "    print(\"avg latency\", avg_latency)    \n",
    "    print()\n",
    "\n",
    "    durations.append(duration_s)\n",
    "    throughputs.append(throughput)\n",
    "    latencies.append(avg_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render_plot(x, y1, y2, x_label, y1_label, y2_label):\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot the first line (throughput)\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(y1_label, color=color)\n",
    "    ax1.plot(x, y1, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Set the x-axis to be log-scaled\n",
    "    ax1.set_xscale('log', base=2)\n",
    "\n",
    "    # Instantiate a second axes that shares the same x-axis\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(y2_label, color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(x, y2, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "render_plot(\n",
    "    batch_sizes,\n",
    "    throughputs,\n",
    "    latencies,\n",
    "    \"Batch Size\",\n",
    "    \"Throughput\",\n",
    "    \"Latency\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The quick brown fox jumped over the\",\n",
    "    \"The rain in Spain falls\",\n",
    "    \"What comes up must\",\n",
    "]\n",
    "\n",
    "# note: padding=True ensures the padding token will be inserted into the tokenized tensors\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# seed the random number generator so our results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 32\n",
    "batch_size = 8\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[0], 100 if i % batch_size == 0 else 10)\n",
    "    for i in range(queue_size)\n",
    "] # 模拟32个队列请求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The quick brown fox jumped over the', 100),\n",
       " ('The quick brown fox jumped over the', 10),\n",
       " ('The quick brown fox jumped over the', 10),\n",
       " ('The quick brown fox jumped over the', 10),\n",
       " ('The quick brown fox jumped over the', 10),\n",
       " ('The quick brown fox jumped over the', 10),\n",
       " ('The quick brown fox jumped over the', 10),\n",
       " ('The quick brown fox jumped over the', 10)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_queue[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将请求队列分成批次\n",
    "batches = [\n",
    "    request_queue[i: i + batch_size]\n",
    "    for i in range(0, len(request_queue), batch_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8: 100%|██████████| 4/4 [00:40<00:00, 10.21s/it, max_tokens=100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration 41.478201389312744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# generate tokens for all batches and record duration\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(batches), desc=f\"bs={batch_size}\") as pbar:\n",
    "    for i, batch in enumerate(batches):\n",
    "        # to accommodate all the requests with our \n",
    "        # current implementation, we take the max of\n",
    "        # all the tokens to generate among the requests\n",
    "        batch_max_tokens = [b[1] for b in batch]\n",
    "        max_tokens = max(batch_max_tokens)\n",
    "        pbar.set_postfix({'max_tokens': max_tokens})\n",
    "        \n",
    "        batch_prompts = [b[0] for b in batch]\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, padding=True, return_tensors=\"pt\")\n",
    "        generate_batch(inputs, max_tokens=max_tokens)\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(\"duration\", duration_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bs=8: 100%|██████████| 32/32 [00:13<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration 13.968080759048462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_batch(requests):\n",
    "    prompts = [r[0] for r in requests]\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    return {\n",
    "        \"position_ids\": position_ids,\n",
    "        \"responses\": copy.copy(prompts),\n",
    "        \"tokens_remaining\": [r[1] for r in requests],\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "def generate_next_token(batch):\n",
    "    inputs = copy.copy(batch)\n",
    "    inputs.pop(\"responses\")\n",
    "    inputs.pop(\"tokens_remaining\")\n",
    "\n",
    "    next_token_ids, past_key_values = generate_batch_tokens_with_past(inputs)\n",
    "    next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "\n",
    "    return get_next_inputs(\n",
    "        batch,\n",
    "        next_token_ids,\n",
    "        past_key_values,\n",
    "        next_tokens\n",
    "    )\n",
    "\n",
    "def get_next_inputs(batch, next_token_ids, past_key_values, next_tokens):\n",
    "    return {\n",
    "        # '-1' here means the remaining elements for this dim\n",
    "        \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "\n",
    "        # increment last, discard the rest\n",
    "        \"position_ids\": batch[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "\n",
    "        # concatenate vector of 1's with shape [batch_size, 1]\n",
    "        \"attention_mask\": torch.cat([\n",
    "            batch[\"attention_mask\"],\n",
    "            torch.ones((next_token_ids.shape[0], 1)),\n",
    "        ], dim=1),\n",
    "\n",
    "        \"past_key_values\": past_key_values,\n",
    "\n",
    "        \"responses\": [\n",
    "            r1 + r2 for r1, r2 in zip(batch[\"responses\"], next_tokens)\n",
    "        ],\n",
    "\n",
    "        \"tokens_remaining\": [\n",
    "            v - 1 for v in batch[\"tokens_remaining\"]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "def merge_batches(batch1, batch2):\n",
    "    # === 1. 计算 attention_mask 的最大长度 ===\n",
    "    attn_mask1 = batch1[\"attention_mask\"]\n",
    "    attn_mask2 = batch2[\"attention_mask\"]\n",
    "    max_seq_len = max(attn_mask1.shape[1], attn_mask2.shape[1])\n",
    "\n",
    "    # === 2. 左侧 padding 到最大长度（填 0）===\n",
    "    padding1 = max_seq_len - attn_mask1.shape[1]\n",
    "    padding2 = max_seq_len - attn_mask2.shape[1]\n",
    "\n",
    "    attn_mask1 = F.pad(attn_mask1, (padding1, 0), \"constant\", 0)\n",
    "    attn_mask2 = F.pad(attn_mask2, (padding2, 0), \"constant\", 0)\n",
    "\n",
    "    # === 3. pad past_key_values（[B, H, S, D]）===\n",
    "    past_kv1 = batch1[\"past_key_values\"]\n",
    "    past_kv2 = batch2[\"past_key_values\"]\n",
    "\n",
    "    padded_kv1 = []\n",
    "    for k, v in past_kv1:\n",
    "        k = F.pad(k, (0, 0, padding1, 0), \"constant\", 0)\n",
    "        v = F.pad(v, (0, 0, padding1, 0), \"constant\", 0)\n",
    "        padded_kv1.append((k, v))\n",
    "\n",
    "    padded_kv2 = []\n",
    "    for k, v in past_kv2:\n",
    "        k = F.pad(k, (0, 0, padding2, 0), \"constant\", 0)\n",
    "        v = F.pad(v, (0, 0, padding2, 0), \"constant\", 0)\n",
    "        padded_kv2.append((k, v))\n",
    "\n",
    "    # === 4. 合并 input_ids、position_ids、attention_mask ===\n",
    "    input_ids = torch.concat([batch1[\"input_ids\"], batch2[\"input_ids\"]], dim=0)\n",
    "    position_ids = torch.concat([batch1[\"position_ids\"], batch2[\"position_ids\"]], dim=0)\n",
    "    attn_mask = torch.concat([attn_mask1, attn_mask2], dim=0)\n",
    "\n",
    "    # === 5. 合并 past_key_values（按层拼 batch 维）===\n",
    "    past_kv = []\n",
    "    for (k1, v1), (k2, v2) in zip(padded_kv1, padded_kv2):\n",
    "        k = torch.concat([k1, k2], dim=0)\n",
    "        v = torch.concat([v1, v2], dim=0)\n",
    "        past_kv.append((k, v))\n",
    "\n",
    "    # === 6. 合并其他字段（responses, tokens_remaining）===\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"position_ids\": position_ids,\n",
    "        \"attention_mask\": attn_mask,\n",
    "        \"past_key_values\": past_kv,\n",
    "        \"responses\": batch1[\"responses\"] + batch2[\"responses\"],\n",
    "        \"tokens_remaining\": batch1[\"tokens_remaining\"] + batch2[\"tokens_remaining\"],\n",
    "    }\n",
    "\n",
    "def filter_batch(batch):\n",
    "    # === 1. 找出 tokens_remaining <= 0 的样本索引 ===\n",
    "    remove_indices = []\n",
    "    for i, tokens_remaining in enumerate(batch[\"tokens_remaining\"]):\n",
    "        if tokens_remaining <= 0:\n",
    "            remove_indices.append(i)\n",
    "\n",
    "    # === 2. 构建布尔掩码，True 表示保留该样本 ===\n",
    "    batch_size = batch[\"input_ids\"].size(0)\n",
    "    mask = torch.ones(batch_size, dtype=torch.bool)\n",
    "    mask[remove_indices] = False\n",
    "\n",
    "    # === 3. 根据掩码过滤 tensor 类型的数据 ===\n",
    "    input_ids = batch[\"input_ids\"][mask]\n",
    "    position_ids = batch[\"position_ids\"][mask]\n",
    "    attention_mask = batch[\"attention_mask\"][mask]\n",
    "\n",
    "    # === 4. 过滤 list 类型的数据 ===\n",
    "    responses = [\n",
    "        r for i, r in enumerate(batch[\"responses\"]) if i not in remove_indices\n",
    "    ]\n",
    "    tokens_remaining = [\n",
    "        v for i, v in enumerate(batch[\"tokens_remaining\"]) if i not in remove_indices\n",
    "    ]\n",
    "\n",
    "    # === 5. 过滤 past_key_values 中的样本 ===\n",
    "    past_key_values = batch[\"past_key_values\"]\n",
    "    new_past_key_values = []\n",
    "    for k, v in past_key_values:\n",
    "        k = k[mask]\n",
    "        v = v[mask]\n",
    "        new_past_key_values.append((k, v))\n",
    "    past_key_values = new_past_key_values\n",
    "\n",
    "    # === 6. 可选优化：左截断 padding，节省计算 ===\n",
    "    if input_ids.size(0) > 0:\n",
    "        # 找出每行 attention_mask 中开头 0（padding）的数量\n",
    "        zero_mask = attention_mask == 0\n",
    "        cumprod = zero_mask.cumprod(dim=1)  # 在遇到 1 后停止累乘\n",
    "        leading_zeros_count = cumprod.sum(dim=1)\n",
    "        min_leading_zeros = torch.min(leading_zeros_count)\n",
    "        truncation_offset = min_leading_zeros.item()\n",
    "\n",
    "        # 截断 attention_mask\n",
    "        attention_mask = attention_mask[:, truncation_offset:]\n",
    "\n",
    "        # 截断 past_key_values 中的 sequence_length 维度\n",
    "        new_past_key_values = []\n",
    "        for k, v in past_key_values:\n",
    "            k = k[:, :, truncation_offset:, :]\n",
    "            v = v[:, :, truncation_offset:, :]\n",
    "            new_past_key_values.append((k, v))\n",
    "        past_key_values = new_past_key_values\n",
    "\n",
    "    # === 7. 返回过滤后的 batch 和被移除的索引列表 ===\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"position_ids\": position_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"past_key_values\": past_key_values,\n",
    "        \"responses\": responses,\n",
    "        \"tokens_remaining\": tokens_remaining,\n",
    "    }, remove_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# seed the random number generator so our results are deterministic\n",
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 32\n",
    "batch_size = 8\n",
    "\n",
    "# requests waiting to be processed\n",
    "# this time requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[0], 100 if i % batch_size == 0 else 10)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(request_queue), desc=f\"bs={batch_size}\") as pbar:\n",
    "    # first, let's seed the initial cached_batch\n",
    "    # with the first `batch_size` inputs\n",
    "    # and run the initial prefill step\n",
    "    batch = init_batch(request_queue[:batch_size])\n",
    "    cached_batch = generate_next_token(batch)\n",
    "    request_queue = request_queue[batch_size:]\n",
    "\n",
    "    # continue until both the request queue is \n",
    "    # fully drained and every input\n",
    "    # within the cached_batch has completed generation\n",
    "    while (\n",
    "        len(request_queue) > 0 or\n",
    "        cached_batch[\"input_ids\"].size(0) > 0\n",
    "    ):\n",
    "        batch_capacity = (\n",
    "            batch_size - cached_batch[\"input_ids\"].size(0)\n",
    "        )\n",
    "        if batch_capacity > 0 and len(request_queue) > 0:\n",
    "            # prefill\n",
    "            new_batch = init_batch(request_queue[:batch_capacity])\n",
    "            new_batch = generate_next_token(new_batch)\n",
    "            request_queue = request_queue[batch_capacity:]\n",
    "\n",
    "            # merge\n",
    "            cached_batch = merge_batches(cached_batch, new_batch)\n",
    "\n",
    "        # decode\n",
    "        cached_batch = generate_next_token(cached_batch)\n",
    "\n",
    "        # remove any inputs that have finished generation\n",
    "        cached_batch, removed_indices = filter_batch(cached_batch)\n",
    "        pbar.update(len(removed_indices))\n",
    "\n",
    "duration_s = time.time() - t0\n",
    "print(\"duration\", duration_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "# fix dtype post quantization to \"pretend\" to be fp32\n",
    "def get_float32_dtype(self):\n",
    "    return torch.float32\n",
    "GPT2Model.dtype = property(get_float32_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510342192"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(t):\n",
    "    # obtain range of values in the tensor to map between 0 and 255\n",
    "    min_val, max_val = t.min(), t.max()\n",
    "\n",
    "    # determine the \"zero-point\", or value in the tensor to map to 0\n",
    "    scale = (max_val - min_val) / 255\n",
    "    zero_point = min_val\n",
    "\n",
    "    # quantize and clamp to ensure we're in [0, 255]\n",
    "    t_quant = (t - zero_point) / scale\n",
    "    t_quant = torch.clamp(t_quant, min=0, max=255)\n",
    "\n",
    "    # keep track of scale and zero_point for reversing quantization\n",
    "    state = (scale, zero_point)\n",
    "\n",
    "    # cast to uint8 and return\n",
    "    t_quant = t_quant.type(torch.uint8)\n",
    "    return t_quant, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
      "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
      "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
      "        ...,\n",
      "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
      "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
      "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]]) torch.Size([768, 2304])\n"
     ]
    }
   ],
   "source": [
    "t = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print(t, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[107, 116, 124,  ..., 130, 125, 129],\n",
      "        [132, 135, 139,  ..., 126, 128, 127],\n",
      "        [128, 131, 145,  ..., 133, 130, 127],\n",
      "        ...,\n",
      "        [116, 127, 137,  ..., 129, 126, 130],\n",
      "        [135, 138, 133,  ..., 129, 126, 126],\n",
      "        [110, 119, 117,  ..., 128, 128, 129]], dtype=torch.uint8) tensor(0, dtype=torch.uint8) tensor(255, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "t_q, state = quantize(t) # int8量化\n",
    "print(t_q, t_q.min(), t_q.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantize(t, state):\n",
    "    scale, zero_point = state\n",
    "    return t.to(torch.float32) * scale + zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4774, -0.2783, -0.1014,  ...,  0.0313, -0.0793,  0.0092],\n",
      "        [ 0.0755,  0.1419,  0.2303,  ..., -0.0572, -0.0129, -0.0351],\n",
      "        [-0.0129,  0.0534,  0.3630,  ...,  0.0976,  0.0313, -0.0351],\n",
      "        ...,\n",
      "        [-0.2783, -0.0351,  0.1861,  ...,  0.0092, -0.0572,  0.0313],\n",
      "        [ 0.1419,  0.2082,  0.0976,  ...,  0.0092, -0.0572, -0.0572],\n",
      "        [-0.4110, -0.2120, -0.2562,  ..., -0.0129, -0.0129,  0.0092]])\n"
     ]
    }
   ],
   "source": [
    "t_rev = dequantize(t_q, state) # 反量化\n",
    "print(t_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0035, 0.0170, 0.0036,  ..., 0.0200, 0.0209, 0.0158],\n",
       "        [0.0119, 0.0055, 0.0084,  ..., 0.0046, 0.0017, 0.0195],\n",
       "        [0.0168, 0.0161, 0.0038,  ..., 0.0167, 0.0050, 0.0032],\n",
       "        ...,\n",
       "        [0.0191, 0.0187, 0.0131,  ..., 0.0004, 0.0056, 0.0006],\n",
       "        [0.0098, 0.0088, 0.0067,  ..., 0.0202, 0.0143, 0.0097],\n",
       "        [0.0010, 0.0196, 0.0162,  ..., 0.0084, 0.0199, 0.0107]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(t - t_rev) # 精度损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  464,  2068,  7586, 21831, 11687,   625,   262]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' fence and ran to the other side of the fence'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"The quick brown fox jumped over the\", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "response_expected = generate(\n",
    "    inputs,\n",
    "    10\n",
    ")\n",
    "response_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 量化整个模型\n",
    "def quantize_model(model):\n",
    "    states = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        param.data, state = quantize(param.data)\n",
    "        states[name] = state\n",
    "    return model, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model, states = quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer.wte.weight': (tensor(0.0120), tensor(-1.2698)),\n",
       " 'transformer.wpe.weight': (tensor(0.0337), tensor(-4.5381)),\n",
       " 'transformer.h.0.ln_1.weight': (tensor(0.0008), tensor(0.0419)),\n",
       " 'transformer.h.0.ln_1.bias': (tensor(0.0018), tensor(-0.2589)),\n",
       " 'transformer.h.0.attn.c_attn.weight': (tensor(0.0221), tensor(-2.8436)),\n",
       " 'transformer.h.0.attn.c_attn.bias': (tensor(0.0099), tensor(-1.3371)),\n",
       " 'transformer.h.0.attn.c_proj.weight': (tensor(0.0250), tensor(-3.3171)),\n",
       " 'transformer.h.0.attn.c_proj.bias': (tensor(0.0185), tensor(-2.6844)),\n",
       " 'transformer.h.0.ln_2.weight': (tensor(0.0057), tensor(0.0453)),\n",
       " 'transformer.h.0.ln_2.bias': (tensor(0.0055), tensor(-0.6648)),\n",
       " 'transformer.h.0.mlp.c_fc.weight': (tensor(0.0271), tensor(-2.3131)),\n",
       " 'transformer.h.0.mlp.c_fc.bias': (tensor(0.0042), tensor(-0.7462)),\n",
       " 'transformer.h.0.mlp.c_proj.weight': (tensor(0.0479), tensor(-6.1433)),\n",
       " 'transformer.h.0.mlp.c_proj.bias': (tensor(0.0098), tensor(-1.0288)),\n",
       " 'transformer.h.1.ln_1.weight': (tensor(0.0023), tensor(0.0725)),\n",
       " 'transformer.h.1.ln_1.bias': (tensor(0.0047), tensor(-0.6645)),\n",
       " 'transformer.h.1.attn.c_attn.weight': (tensor(0.0091), tensor(-1.0771)),\n",
       " 'transformer.h.1.attn.c_attn.bias': (tensor(0.0147), tensor(-1.8151)),\n",
       " 'transformer.h.1.attn.c_proj.weight': (tensor(0.0342), tensor(-4.7262)),\n",
       " 'transformer.h.1.attn.c_proj.bias': (tensor(0.0070), tensor(-0.5346)),\n",
       " 'transformer.h.1.ln_2.weight': (tensor(0.0016), tensor(0.0560)),\n",
       " 'transformer.h.1.ln_2.bias': (tensor(0.0041), tensor(-0.5866)),\n",
       " 'transformer.h.1.mlp.c_fc.weight': (tensor(0.0163), tensor(-1.8728)),\n",
       " 'transformer.h.1.mlp.c_fc.bias': (tensor(0.0036), tensor(-0.6563)),\n",
       " 'transformer.h.1.mlp.c_proj.weight': (tensor(0.0732), tensor(-4.9305)),\n",
       " 'transformer.h.1.mlp.c_proj.bias': (tensor(0.0090), tensor(-0.6991)),\n",
       " 'transformer.h.2.ln_1.weight': (tensor(0.0035), tensor(0.0460)),\n",
       " 'transformer.h.2.ln_1.bias': (tensor(0.0065), tensor(-0.5625)),\n",
       " 'transformer.h.2.attn.c_attn.weight': (tensor(0.0128), tensor(-1.6774)),\n",
       " 'transformer.h.2.attn.c_attn.bias': (tensor(0.0104), tensor(-1.4630)),\n",
       " 'transformer.h.2.attn.c_proj.weight': (tensor(0.0177), tensor(-2.2955)),\n",
       " 'transformer.h.2.attn.c_proj.bias': (tensor(0.0039), tensor(-0.4775)),\n",
       " 'transformer.h.2.ln_2.weight': (tensor(0.0027), tensor(0.0421)),\n",
       " 'transformer.h.2.ln_2.bias': (tensor(0.0040), tensor(-0.6470)),\n",
       " 'transformer.h.2.mlp.c_fc.weight': (tensor(0.0644), tensor(-5.8740)),\n",
       " 'transformer.h.2.mlp.c_fc.bias': (tensor(0.0094), tensor(-0.6598)),\n",
       " 'transformer.h.2.mlp.c_proj.weight': (tensor(0.0704), tensor(-2.8738)),\n",
       " 'transformer.h.2.mlp.c_proj.bias': (tensor(0.0079), tensor(-0.4528)),\n",
       " 'transformer.h.3.ln_1.weight': (tensor(0.0028), tensor(0.0565)),\n",
       " 'transformer.h.3.ln_1.bias': (tensor(0.0085), tensor(-0.4335)),\n",
       " 'transformer.h.3.attn.c_attn.weight': (tensor(0.0142), tensor(-1.7154)),\n",
       " 'transformer.h.3.attn.c_attn.bias': (tensor(0.0054), tensor(-0.6688)),\n",
       " 'transformer.h.3.attn.c_proj.weight': (tensor(0.0130), tensor(-2.0938)),\n",
       " 'transformer.h.3.attn.c_proj.bias': (tensor(0.0061), tensor(-1.0272)),\n",
       " 'transformer.h.3.ln_2.weight': (tensor(0.0046), tensor(-0.0003)),\n",
       " 'transformer.h.3.ln_2.bias': (tensor(0.0034), tensor(-0.4375)),\n",
       " 'transformer.h.3.mlp.c_fc.weight': (tensor(0.0193), tensor(-2.6437)),\n",
       " 'transformer.h.3.mlp.c_fc.bias': (tensor(0.0068), tensor(-1.2262)),\n",
       " 'transformer.h.3.mlp.c_proj.weight': (tensor(0.0833), tensor(-4.1391)),\n",
       " 'transformer.h.3.mlp.c_proj.bias': (tensor(0.0099), tensor(-0.6661)),\n",
       " 'transformer.h.4.ln_1.weight': (tensor(0.0024), tensor(0.0580)),\n",
       " 'transformer.h.4.ln_1.bias': (tensor(0.0082), tensor(-0.5446)),\n",
       " 'transformer.h.4.attn.c_attn.weight': (tensor(0.0252), tensor(-3.3341)),\n",
       " 'transformer.h.4.attn.c_attn.bias': (tensor(0.0210), tensor(-2.6123)),\n",
       " 'transformer.h.4.attn.c_proj.weight': (tensor(0.0144), tensor(-1.8388)),\n",
       " 'transformer.h.4.attn.c_proj.bias': (tensor(0.0045), tensor(-0.6430)),\n",
       " 'transformer.h.4.ln_2.weight': (tensor(0.0042), tensor(0.0701)),\n",
       " 'transformer.h.4.ln_2.bias': (tensor(0.0011), tensor(-0.1421)),\n",
       " 'transformer.h.4.mlp.c_fc.weight': (tensor(0.0164), tensor(-2.1614)),\n",
       " 'transformer.h.4.mlp.c_fc.bias': (tensor(0.0047), tensor(-0.4530)),\n",
       " 'transformer.h.4.mlp.c_proj.weight': (tensor(0.0334), tensor(-3.7426)),\n",
       " 'transformer.h.4.mlp.c_proj.bias': (tensor(0.0088), tensor(-0.6683)),\n",
       " 'transformer.h.5.ln_1.weight': (tensor(0.0027), tensor(0.0864)),\n",
       " 'transformer.h.5.ln_1.bias': (tensor(0.0060), tensor(-0.4506)),\n",
       " 'transformer.h.5.attn.c_attn.weight': (tensor(0.0107), tensor(-1.3924)),\n",
       " 'transformer.h.5.attn.c_attn.bias': (tensor(0.0041), tensor(-0.5594)),\n",
       " 'transformer.h.5.attn.c_proj.weight': (tensor(0.0144), tensor(-1.9833)),\n",
       " 'transformer.h.5.attn.c_proj.bias': (tensor(0.0051), tensor(-0.7030)),\n",
       " 'transformer.h.5.ln_2.weight': (tensor(0.0054), tensor(0.0428)),\n",
       " 'transformer.h.5.ln_2.bias': (tensor(0.0024), tensor(-0.3045)),\n",
       " 'transformer.h.5.mlp.c_fc.weight': (tensor(0.0146), tensor(-1.9646)),\n",
       " 'transformer.h.5.mlp.c_fc.bias': (tensor(0.0043), tensor(-0.4335)),\n",
       " 'transformer.h.5.mlp.c_proj.weight': (tensor(0.0219), tensor(-2.8407)),\n",
       " 'transformer.h.5.mlp.c_proj.bias': (tensor(0.0077), tensor(-0.7120)),\n",
       " 'transformer.h.6.ln_1.weight': (tensor(0.0028), tensor(0.0637)),\n",
       " 'transformer.h.6.ln_1.bias': (tensor(0.0085), tensor(-0.6354)),\n",
       " 'transformer.h.6.attn.c_attn.weight': (tensor(0.0118), tensor(-1.3877)),\n",
       " 'transformer.h.6.attn.c_attn.bias': (tensor(0.0060), tensor(-0.8114)),\n",
       " 'transformer.h.6.attn.c_proj.weight': (tensor(0.0140), tensor(-1.7240)),\n",
       " 'transformer.h.6.attn.c_proj.bias': (tensor(0.0028), tensor(-0.3124)),\n",
       " 'transformer.h.6.ln_2.weight': (tensor(0.0051), tensor(0.0424)),\n",
       " 'transformer.h.6.ln_2.bias': (tensor(0.0028), tensor(-0.2726)),\n",
       " 'transformer.h.6.mlp.c_fc.weight': (tensor(0.0132), tensor(-1.2295)),\n",
       " 'transformer.h.6.mlp.c_fc.bias': (tensor(0.0044), tensor(-0.4310)),\n",
       " 'transformer.h.6.mlp.c_proj.weight': (tensor(0.0197), tensor(-2.2441)),\n",
       " 'transformer.h.6.mlp.c_proj.bias': (tensor(0.0068), tensor(-0.6757)),\n",
       " 'transformer.h.7.ln_1.weight': (tensor(0.0029), tensor(0.0746)),\n",
       " 'transformer.h.7.ln_1.bias': (tensor(0.0078), tensor(-0.7903)),\n",
       " 'transformer.h.7.attn.c_attn.weight': (tensor(0.0137), tensor(-1.6830)),\n",
       " 'transformer.h.7.attn.c_attn.bias': (tensor(0.0058), tensor(-0.7567)),\n",
       " 'transformer.h.7.attn.c_proj.weight': (tensor(0.0163), tensor(-2.2344)),\n",
       " 'transformer.h.7.attn.c_proj.bias': (tensor(0.0038), tensor(-0.4598)),\n",
       " 'transformer.h.7.ln_2.weight': (tensor(0.0050), tensor(0.0149)),\n",
       " 'transformer.h.7.ln_2.bias': (tensor(0.0043), tensor(-0.4666)),\n",
       " 'transformer.h.7.mlp.c_fc.weight': (tensor(0.0082), tensor(-0.8502)),\n",
       " 'transformer.h.7.mlp.c_fc.bias': (tensor(0.0061), tensor(-0.7277)),\n",
       " 'transformer.h.7.mlp.c_proj.weight': (tensor(0.0346), tensor(-4.5328)),\n",
       " 'transformer.h.7.mlp.c_proj.bias': (tensor(0.0075), tensor(-0.7256)),\n",
       " 'transformer.h.8.ln_1.weight': (tensor(0.0034), tensor(0.0657)),\n",
       " 'transformer.h.8.ln_1.bias': (tensor(0.0093), tensor(-0.9212)),\n",
       " 'transformer.h.8.attn.c_attn.weight': (tensor(0.0144), tensor(-1.7242)),\n",
       " 'transformer.h.8.attn.c_attn.bias': (tensor(0.0067), tensor(-0.8690)),\n",
       " 'transformer.h.8.attn.c_proj.weight': (tensor(0.0201), tensor(-2.1775)),\n",
       " 'transformer.h.8.attn.c_proj.bias': (tensor(0.0064), tensor(-0.4676)),\n",
       " 'transformer.h.8.ln_2.weight': (tensor(0.0041), tensor(0.0180)),\n",
       " 'transformer.h.8.ln_2.bias': (tensor(0.0045), tensor(-0.6950)),\n",
       " 'transformer.h.8.mlp.c_fc.weight': (tensor(0.0105), tensor(-1.2222)),\n",
       " 'transformer.h.8.mlp.c_fc.bias': (tensor(0.0060), tensor(-0.5317)),\n",
       " 'transformer.h.8.mlp.c_proj.weight': (tensor(0.0354), tensor(-3.6508)),\n",
       " 'transformer.h.8.mlp.c_proj.bias': (tensor(0.0080), tensor(-0.8268)),\n",
       " 'transformer.h.9.ln_1.weight': (tensor(0.0034), tensor(0.0696)),\n",
       " 'transformer.h.9.ln_1.bias': (tensor(0.0088), tensor(-0.9785)),\n",
       " 'transformer.h.9.attn.c_attn.weight': (tensor(0.0148), tensor(-1.7891)),\n",
       " 'transformer.h.9.attn.c_attn.bias': (tensor(0.0072), tensor(-1.0424)),\n",
       " 'transformer.h.9.attn.c_proj.weight': (tensor(0.0143), tensor(-1.6596)),\n",
       " 'transformer.h.9.attn.c_proj.bias': (tensor(0.0112), tensor(-0.9575)),\n",
       " 'transformer.h.9.ln_2.weight': (tensor(0.0036), tensor(0.0177)),\n",
       " 'transformer.h.9.ln_2.bias': (tensor(0.0042), tensor(-0.5610)),\n",
       " 'transformer.h.9.mlp.c_fc.weight': (tensor(0.0192), tensor(-2.7818)),\n",
       " 'transformer.h.9.mlp.c_fc.bias': (tensor(0.0043), tensor(-0.4776)),\n",
       " 'transformer.h.9.mlp.c_proj.weight': (tensor(0.0406), tensor(-5.4875)),\n",
       " 'transformer.h.9.mlp.c_proj.bias': (tensor(0.0109), tensor(-1.2830)),\n",
       " 'transformer.h.10.ln_1.weight': (tensor(0.0033), tensor(0.0751)),\n",
       " 'transformer.h.10.ln_1.bias': (tensor(0.0068), tensor(-0.6528)),\n",
       " 'transformer.h.10.attn.c_attn.weight': (tensor(0.0145), tensor(-1.8938)),\n",
       " 'transformer.h.10.attn.c_attn.bias': (tensor(0.0066), tensor(-0.9162)),\n",
       " 'transformer.h.10.attn.c_proj.weight': (tensor(0.0325), tensor(-4.0754)),\n",
       " 'transformer.h.10.attn.c_proj.bias': (tensor(0.0268), tensor(-2.9879)),\n",
       " 'transformer.h.10.ln_2.weight': (tensor(0.0042), tensor(0.0201)),\n",
       " 'transformer.h.10.ln_2.bias': (tensor(0.0051), tensor(-0.6005)),\n",
       " 'transformer.h.10.mlp.c_fc.weight': (tensor(0.0184), tensor(-2.5535)),\n",
       " 'transformer.h.10.mlp.c_fc.bias': (tensor(0.0070), tensor(-1.0728)),\n",
       " 'transformer.h.10.mlp.c_proj.weight': (tensor(0.0798), tensor(-11.0504)),\n",
       " 'transformer.h.10.mlp.c_proj.bias': (tensor(0.0095), tensor(-1.0768)),\n",
       " 'transformer.h.11.ln_1.weight': (tensor(0.0033), tensor(0.1061)),\n",
       " 'transformer.h.11.ln_1.bias': (tensor(0.0052), tensor(-0.3304)),\n",
       " 'transformer.h.11.attn.c_attn.weight': (tensor(0.0179), tensor(-2.2638)),\n",
       " 'transformer.h.11.attn.c_attn.bias': (tensor(0.0055), tensor(-0.8046)),\n",
       " 'transformer.h.11.attn.c_proj.weight': (tensor(0.0690), tensor(-8.8818)),\n",
       " 'transformer.h.11.attn.c_proj.bias': (tensor(0.0353), tensor(-5.3729)),\n",
       " 'transformer.h.11.ln_2.weight': (tensor(0.0047), tensor(0.0278)),\n",
       " 'transformer.h.11.ln_2.bias': (tensor(0.0025), tensor(-0.2093)),\n",
       " 'transformer.h.11.mlp.c_fc.weight': (tensor(0.0143), tensor(-1.9649)),\n",
       " 'transformer.h.11.mlp.c_fc.bias': (tensor(0.0068), tensor(-1.2082)),\n",
       " 'transformer.h.11.mlp.c_proj.weight': (tensor(0.0720), tensor(-9.2115)),\n",
       " 'transformer.h.11.mlp.c_proj.bias': (tensor(0.0032), tensor(-0.3835)),\n",
       " 'transformer.ln_f.weight': (tensor(0.0683), tensor(0.0044)),\n",
       " 'transformer.ln_f.bias': (tensor(0.0453), tensor(-4.1918))}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137022768"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.get_memory_footprint() # 从0.4GB降到0.1GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_in_bytes(t): # 计算张量 t 在内存中实际占用了多少字节（bytes）\n",
    "    return t.numel() * t.element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1184"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([\n",
    "    size_in_bytes(v[0]) + size_in_bytes(v[1])\n",
    "    for v in states.values()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反量化整个模型\n",
    "def dequantize_model(model, states):\n",
    "    for name, param in model.named_parameters():\n",
    "        state = states[name]\n",
    "        param.data = dequantize(param.data, state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequant_model = dequantize_model(quant_model, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510342192"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dequant_model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x73a6b81080d0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(10, hidden_size)\n",
    "        self.linear = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.lm_head = torch.nn.Linear(hidden_size, 10)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.linear(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a reasonably large hidden size to illustrate the small fraction of\n",
    "# params needed to be added for LoRA\n",
    "hidden_size = 1024\n",
    "model = TestModel(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy inputs\n",
    "input_ids = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example of a detokenizer. \n",
    "# The vocabulary only consists of 10 words (different colors)\n",
    "detokenizer = [\n",
    "    \"red\",\n",
    "    \"orange\",\n",
    "    \"yellow\",\n",
    "    \"green\",\n",
    "    \"blue\",\n",
    "    \"indigo\",\n",
    "    \"violet\",\n",
    "    \"magenta\",\n",
    "    \"marigold\",\n",
    "    \"chartreuse\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token(model, **kwargs):\n",
    "    with torch.no_grad():\n",
    "        logits = model(**kwargs)\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "\n",
    "    return [detokenizer[token_id] for token_id in next_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'magenta'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate one token\n",
    "next_token = generate_token(model, input_ids=input_ids)[0]\n",
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy input tensor\n",
    "# shape: (batch_size, sequence_length, hidden_size)\n",
    "X = torch.randn(1, 8, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA A and B tensors\n",
    "# A has shape (hidden_size, rank)\n",
    "# B has shape (rank, hidden_size)\n",
    "lora_a = torch.randn(1024, 2)\n",
    "lora_b = torch.randn(2, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = lora_a @ lora_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1024])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|A+B| / |W|: 0.00390625\n"
     ]
    }
   ],
   "source": [
    "# Compare number of elements of A and B with number of elements of W\n",
    "# W here has shape (hidden_size, hidden_size)\n",
    "lora_numel = lora_a.numel() + lora_b.numel()\n",
    "base_numel = W.numel()\n",
    "print(\"|A+B| / |W|:\", lora_numel / base_numel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1024])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the output of X @ W (the original linear layer)\n",
    "base_output = model.linear(X)\n",
    "\n",
    "# compute the output of X @ A @ B (the added lora adapter)\n",
    "lora_output = X @ lora_a @ lora_b\n",
    "\n",
    "# sum them together\n",
    "total_output = base_output + lora_output\n",
    "\n",
    "# output should have the same shape as the original output:\n",
    "# (batch_size, sequence_length, hidden_size)\n",
    "total_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLayer(torch.nn.Module):\n",
    "    def __init__(self, base_layer, r):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        \n",
    "        d_in, d_out = self.base_layer.weight.shape\n",
    "        self.lora_a = torch.randn(d_in, r)\n",
    "        self.lora_b = torch.randn(r, d_out) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y1 = self.base_layer(x)\n",
    "        y2 = x @ self.lora_a @ self.lora_b\n",
    "        return y1 + y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1024])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrap the linear layer of our toy model, use rank 2\n",
    "lora_layer = LoraLayer(model.linear, 2)\n",
    "lora_layer(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear = lora_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestModel(\n",
       "  (embedding): Embedding(10, 1024)\n",
       "  (linear): LoraLayer(\n",
       "    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indigo'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = generate_token(model, input_ids=input_ids)\n",
    "next_token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Lora inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractMultiLoraModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # hidden_size = 10\n",
    "        # set this so low to ensure we are not \n",
    "        # compute-bound by the linear layer\n",
    "        # this is only an issue when running on CPU, \n",
    "        # for GPUs we can set this much\n",
    "        # higher and still avoid being compute bound\n",
    "        self.embedding = torch.nn.Embedding(10, 10)\n",
    "        self.linear = torch.nn.Linear(10, 10)\n",
    "        self.lm_head = torch.nn.Linear(10, 10)\n",
    "\n",
    "    def linear_lora(\n",
    "        self,\n",
    "        x: torch.Tensor,                 # (batch_size, seq_len, in_features)\n",
    "        loras_a: torch.Tensor,           # (num_loras, in_features, rank)\n",
    "        loras_b: torch.Tensor,           # (num_loras, rank, out_features)\n",
    "        lora_indices: torch.LongTensor,  # (batch_size,)\n",
    "    ) -> torch.Tensor:\n",
    "        # y[i] = x[i] @ loras_a[lora_idx] @ loras_b[lora_idx]\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_ids, loras_a, loras_b, lora_indices):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.linear_lora(x, loras_a, loras_b, lora_indices)\n",
    "        x = self.lm_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopMultiLoraModel(AbstractMultiLoraModel):\n",
    "    def linear_lora(\n",
    "        self,\n",
    "        x: torch.Tensor,                 # (batch_size, seq_len, in_features)\n",
    "        loras_a: torch.Tensor,           # (num_loras, in_features, lora_rank)\n",
    "        loras_b: torch.Tensor,           # (num_loras, lora_rank, out_features)\n",
    "        lora_indices: torch.LongTensor,  # (batch_size,)\n",
    "    ) -> torch.Tensor:\n",
    "        y = self.linear(x)\n",
    "        for batch_idx, lora_idx in enumerate(lora_indices.numpy()):\n",
    "            lora_a = loras_a[lora_idx]\n",
    "            lora_b = loras_b[lora_idx]\n",
    "            y[batch_idx] += x[batch_idx] @ lora_a @ lora_b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example of a detokenizer. The vocabular only consists of 10 words (different colors)\n",
    "detokenizer = [\n",
    "    \"red\",\n",
    "    \"orange\",\n",
    "    \"yellow\",\n",
    "    \"green\",\n",
    "    \"blue\",\n",
    "    \"indigo\",\n",
    "    \"violet\",\n",
    "    \"magenta\",\n",
    "    \"marigold\",\n",
    "    \"chartreuse\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy inputs\n",
    "input_ids = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
    "torch.manual_seed(42)\n",
    "def generate_token(model, **kwargs):\n",
    "    with torch.no_grad():\n",
    "        logits = model(**kwargs)\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "\n",
    "    return [detokenizer[token_id] for token_id in next_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LoopMultiLoraModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orange']\n",
      "['red']\n",
      "['chartreuse']\n",
      "['yellow']\n",
      "['marigold']\n",
      "['yellow']\n",
      "['red']\n",
      "['orange']\n",
      "['violet']\n",
      "['red']\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "bs = 1\n",
    "num_loras = 64\n",
    "h = 10\n",
    "r = 2\n",
    "\n",
    "# create contiguous blocks for 64 random LoRA weights\n",
    "loras_a = torch.randn(num_loras, h, r)\n",
    "loras_b = torch.randn(num_loras, r, h)\n",
    "\n",
    "for i in range(10):\n",
    "    # randomize the LoRAs each iteration\n",
    "    lora_indices = torch.randint(num_loras, (bs,), dtype=torch.long)\n",
    "    next_token = generate_token(\n",
    "        model,\n",
    "        input_ids=input_ids,\n",
    "        loras_a=loras_a,\n",
    "        loras_b=loras_b,\n",
    "        lora_indices=lora_indices,\n",
    "    )\n",
    "    print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "seq_len = 8\n",
    "vocab_size = 10\n",
    "nsamples = 500\n",
    "max_batch_size = 64\n",
    "\n",
    "\n",
    "def benchmark(model):\n",
    "    avg_latencies = []\n",
    "    for bs in range(1, max_batch_size + 1):\n",
    "        latencies = []\n",
    "        for _ in range(nsamples):\n",
    "            # randomize the inputs and LoRA indices\n",
    "            input_ids = torch.randint(\n",
    "                vocab_size, (bs, seq_len), dtype=torch.long)\n",
    "            lora_indices = torch.randint(\n",
    "                num_loras, (bs,), dtype=torch.long)\n",
    "\n",
    "            # measure the end-to-end latency for \n",
    "            # generating a single token\n",
    "            t0 = time.time()\n",
    "            next_token = generate_token(\n",
    "                model,\n",
    "                input_ids=input_ids,\n",
    "                loras_a=loras_a,\n",
    "                loras_b=loras_b,\n",
    "                lora_indices=lora_indices,\n",
    "            )\n",
    "            latencies.append(time.time() - t0)\n",
    "\n",
    "        # average the latency across all the samples\n",
    "        latency_s = sum(latencies) / len(latencies)\n",
    "        avg_latencies.append(latency_s)\n",
    "        print(bs, latency_s)\n",
    "    return avg_latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_latencies_loop = benchmark(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRAX\n",
    "- LLM 推理适配器服务框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lorax-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pydantic import BaseModel, constr\n",
    "\n",
    "from lorax import AsyncClient, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(base_url=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pptagent/lib/python3.11/http/client.py:1298\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pptagent/lib/python3.11/http/client.py:1058\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1058\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pptagent/lib/python3.11/http/client.py:996\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 996\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x73a598e73f10>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# kv cache\u001b[39;00m\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is deep learning?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m duration_s \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(resp\u001b[38;5;241m.\u001b[39mgenerated_text)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/lorax/client.py:265\u001b[0m, in \u001b[0;36mClient.generate\u001b[0;34m(self, prompt, adapter_id, adapter_source, merged_adapters, api_token, do_sample, max_new_tokens, ignore_eos_token, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, response_format, decoder_input_details, return_k_alternatives, details)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Instantiate the request object\u001b[39;00m\n\u001b[1;32m    263\u001b[0m request \u001b[38;5;241m=\u001b[39m Request(inputs\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, parameters\u001b[38;5;241m=\u001b[39mparameters)\n\u001b[0;32m--> 265\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     payload \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/lorax/client.py:121\u001b[0m, in \u001b[0;36mClient._post\u001b[0;34m(self, json, stream)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resp\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectTimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;66;03m# Refresh session if there is a ConnectionError\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/connectionpool.py:871\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    868\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    870\u001b[0m     )\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    890\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/connectionpool.py:871\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    868\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    870\u001b[0m     )\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    890\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m    841\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    842\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    843\u001b[0m )\n\u001b[0;32m--> 844\u001b[0m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n\u001b[1;32m    847\u001b[0m err \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/util/retry.py:363\u001b[0m, in \u001b[0;36mRetry.sleep\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m slept:\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sleep_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/urllib3/util/retry.py:347\u001b[0m, in \u001b[0;36mRetry._sleep_backoff\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backoff \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# kv cache\n",
    "t0 = time.time()\n",
    "resp = client.generate(\"What is deep learning?\", max_new_tokens=32)\n",
    "duration_s = time.time() - t0\n",
    "\n",
    "print(resp.generated_text)\n",
    "print(\"\\n\\n----------\")\n",
    "print(\"Request duration (s):\", duration_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pptagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
