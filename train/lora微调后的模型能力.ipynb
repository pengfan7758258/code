{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f27382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_name: qwen2.5-7b-instruct\n",
    "dataset: 1k 小学生数学gpt-4o生成数据集\n",
    "\n",
    "llama-factory训练框架，train_lora/llama3_lora_sft.yaml模版修改\n",
    "lora_sft:\n",
    "    template：alpaca（因为dataset数据格式选择的这个，但是应该使用qwen的指令微调模版）\n",
    "    其余超参数未修改\n",
    "\n",
    "train_log:\n",
    "{\"current_steps\": 10, \"total_steps\": 375, \"loss\": 2.2445, \"lr\": 2.368421052631579e-05, \"epoch\": 0.08, \"percentage\": 2.67, \"elapsed_time\": \"0:01:22\", \"remaining_time\": \"0:50:13\"}\n",
    "{\"current_steps\": 20, \"total_steps\": 375, \"loss\": 2.0776, \"lr\": 5e-05, \"epoch\": 0.16, \"percentage\": 5.33, \"elapsed_time\": \"0:02:50\", \"remaining_time\": \"0:50:29\"}\n",
    "{\"current_steps\": 30, \"total_steps\": 375, \"loss\": 1.8661, \"lr\": 7.631578947368422e-05, \"epoch\": 0.24, \"percentage\": 8.0, \"elapsed_time\": \"0:04:12\", \"remaining_time\": \"0:48:18\"}\n",
    "{\"current_steps\": 40, \"total_steps\": 375, \"loss\": 1.5748, \"lr\": 9.999782741484788e-05, \"epoch\": 0.32, \"percentage\": 10.67, \"elapsed_time\": \"0:05:38\", \"remaining_time\": \"0:47:11\"}\n",
    "{\"current_steps\": 50, \"total_steps\": 375, \"loss\": 1.3094, \"lr\": 9.973734557329009e-05, \"epoch\": 0.4, \"percentage\": 13.33, \"elapsed_time\": \"0:07:04\", \"remaining_time\": \"0:45:58\"}\n",
    "{\"current_steps\": 60, \"total_steps\": 375, \"loss\": 1.3227, \"lr\": 9.904493906342123e-05, \"epoch\": 0.48, \"percentage\": 16.0, \"elapsed_time\": \"0:08:30\", \"remaining_time\": \"0:44:42\"}\n",
    "{\"current_steps\": 70, \"total_steps\": 375, \"loss\": 1.1884, \"lr\": 9.792662082076618e-05, \"epoch\": 0.56, \"percentage\": 18.67, \"elapsed_time\": \"0:09:56\", \"remaining_time\": \"0:43:20\"}\n",
    "{\"current_steps\": 80, \"total_steps\": 375, \"loss\": 1.1047, \"lr\": 9.639210244594334e-05, \"epoch\": 0.64, \"percentage\": 21.33, \"elapsed_time\": \"0:11:17\", \"remaining_time\": \"0:41:36\"}\n",
    "{\"current_steps\": 90, \"total_steps\": 375, \"loss\": 1.1267, \"lr\": 9.445470986803922e-05, \"epoch\": 0.72, \"percentage\": 24.0, \"elapsed_time\": \"0:12:39\", \"remaining_time\": \"0:40:04\"}\n",
    "{\"current_steps\": 100, \"total_steps\": 375, \"loss\": 1.1289, \"lr\": 9.213126762075088e-05, \"epoch\": 0.8, \"percentage\": 26.67, \"elapsed_time\": \"0:13:58\", \"remaining_time\": \"0:38:24\"}\n",
    "{\"current_steps\": 110, \"total_steps\": 375, \"loss\": 1.0804, \"lr\": 8.94419527362547e-05, \"epoch\": 0.88, \"percentage\": 29.33, \"elapsed_time\": \"0:15:18\", \"remaining_time\": \"0:36:52\"}\n",
    "{\"current_steps\": 120, \"total_steps\": 375, \"loss\": 1.0186, \"lr\": 8.641011952560371e-05, \"epoch\": 0.96, \"percentage\": 32.0, \"elapsed_time\": \"0:16:39\", \"remaining_time\": \"0:35:24\"}\n",
    "{\"current_steps\": 130, \"total_steps\": 375, \"loss\": 1.0905, \"lr\": 8.306209676727994e-05, \"epoch\": 1.04, \"percentage\": 34.67, \"elapsed_time\": \"0:18:08\", \"remaining_time\": \"0:34:11\"}\n",
    "{\"current_steps\": 140, \"total_steps\": 375, \"loss\": 1.1956, \"lr\": 7.94269590651393e-05, \"epoch\": 1.12, \"percentage\": 37.33, \"elapsed_time\": \"0:19:30\", \"remaining_time\": \"0:32:44\"}\n",
    "{\"current_steps\": 150, \"total_steps\": 375, \"loss\": 1.1192, \"lr\": 7.553627436130183e-05, \"epoch\": 1.2, \"percentage\": 40.0, \"elapsed_time\": \"0:20:53\", \"remaining_time\": \"0:31:20\"}\n",
    "{\"current_steps\": 160, \"total_steps\": 375, \"loss\": 1.0859, \"lr\": 7.142382979661386e-05, \"epoch\": 1.28, \"percentage\": 42.67, \"elapsed_time\": \"0:22:17\", \"remaining_time\": \"0:29:57\"}\n",
    "{\"current_steps\": 170, \"total_steps\": 375, \"loss\": 1.1021, \"lr\": 6.712533829934042e-05, \"epoch\": 1.3599999999999999, \"percentage\": 45.33, \"elapsed_time\": \"0:23:43\", \"remaining_time\": \"0:28:36\"}\n",
    "{\"current_steps\": 180, \"total_steps\": 375, \"loss\": 1.1415, \"lr\": 6.26781284501043e-05, \"epoch\": 1.44, \"percentage\": 48.0, \"elapsed_time\": \"0:24:59\", \"remaining_time\": \"0:27:04\"}\n",
    "{\"current_steps\": 190, \"total_steps\": 375, \"loss\": 1.0146, \"lr\": 5.812082031631966e-05, \"epoch\": 1.52, \"percentage\": 50.67, \"elapsed_time\": \"0:26:22\", \"remaining_time\": \"0:25:41\"}\n",
    "{\"current_steps\": 200, \"total_steps\": 375, \"loss\": 1.2343, \"lr\": 5.3492990071209806e-05, \"epoch\": 1.6, \"percentage\": 53.33, \"elapsed_time\": \"0:27:50\", \"remaining_time\": \"0:24:21\"}\n",
    "{\"current_steps\": 210, \"total_steps\": 375, \"loss\": 1.1766, \"lr\": 4.883482630989535e-05, \"epoch\": 1.6800000000000002, \"percentage\": 56.0, \"elapsed_time\": \"0:29:13\", \"remaining_time\": \"0:22:57\"}\n",
    "{\"current_steps\": 220, \"total_steps\": 375, \"loss\": 1.0366, \"lr\": 4.418678104714214e-05, \"epoch\": 1.76, \"percentage\": 58.67, \"elapsed_time\": \"0:30:39\", \"remaining_time\": \"0:21:35\"}\n",
    "{\"current_steps\": 230, \"total_steps\": 375, \"loss\": 1.0272, \"lr\": 3.958921842754351e-05, \"epoch\": 1.8399999999999999, \"percentage\": 61.33, \"elapsed_time\": \"0:32:01\", \"remaining_time\": \"0:20:11\"}\n",
    "{\"current_steps\": 240, \"total_steps\": 375, \"loss\": 1.1041, \"lr\": 3.5082064198777e-05, \"epoch\": 1.92, \"percentage\": 64.0, \"elapsed_time\": \"0:33:25\", \"remaining_time\": \"0:18:48\"}\n",
    "{\"current_steps\": 250, \"total_steps\": 375, \"loss\": 1.0889, \"lr\": 3.070445899194885e-05, \"epoch\": 2.0, \"percentage\": 66.67, \"elapsed_time\": \"0:34:48\", \"remaining_time\": \"0:17:24\"}\n",
    "{\"current_steps\": 260, \"total_steps\": 375, \"loss\": 1.0475, \"lr\": 2.6494418419978482e-05, \"epoch\": 2.08, \"percentage\": 69.33, \"elapsed_time\": \"0:36:07\", \"remaining_time\": \"0:15:58\"}\n",
    "{\"current_steps\": 270, \"total_steps\": 375, \"loss\": 1.0926, \"lr\": 2.2488502945766894e-05, \"epoch\": 2.16, \"percentage\": 72.0, \"elapsed_time\": \"0:37:30\", \"remaining_time\": \"0:14:35\"}\n",
    "{\"current_steps\": 280, \"total_steps\": 375, \"loss\": 1.0669, \"lr\": 1.872150038705015e-05, \"epoch\": 2.24, \"percentage\": 74.67, \"elapsed_time\": \"0:38:51\", \"remaining_time\": \"0:13:10\"}\n",
    "{\"current_steps\": 290, \"total_steps\": 375, \"loss\": 1.0358, \"lr\": 1.5226123815101951e-05, \"epoch\": 2.32, \"percentage\": 77.33, \"elapsed_time\": \"0:40:09\", \"remaining_time\": \"0:11:46\"}\n",
    "{\"current_steps\": 300, \"total_steps\": 375, \"loss\": 1.0933, \"lr\": 1.203272747076598e-05, \"epoch\": 2.4, \"percentage\": 80.0, \"elapsed_time\": \"0:41:36\", \"remaining_time\": \"0:10:24\"}\n",
    "{\"current_steps\": 310, \"total_steps\": 375, \"loss\": 1.073, \"lr\": 9.169043164835867e-06, \"epoch\": 2.48, \"percentage\": 82.67, \"elapsed_time\": \"0:43:03\", \"remaining_time\": \"0:09:01\"}\n",
    "{\"current_steps\": 320, \"total_steps\": 375, \"loss\": 1.0171, \"lr\": 6.659939451910341e-06, \"epoch\": 2.56, \"percentage\": 85.33, \"elapsed_time\": \"0:44:27\", \"remaining_time\": \"0:07:38\"}\n",
    "{\"current_steps\": 330, \"total_steps\": 375, \"loss\": 1.0291, \"lr\": 4.527205669085549e-06, \"epoch\": 2.64, \"percentage\": 88.0, \"elapsed_time\": \"0:45:51\", \"remaining_time\": \"0:06:15\"}\n",
    "{\"current_steps\": 340, \"total_steps\": 375, \"loss\": 1.1559, \"lr\": 2.7893627149161716e-06, \"epoch\": 2.7199999999999998, \"percentage\": 90.67, \"elapsed_time\": \"0:47:14\", \"remaining_time\": \"0:04:51\"}\n",
    "{\"current_steps\": 350, \"total_steps\": 375, \"loss\": 1.1794, \"lr\": 1.4615022118622367e-06, \"epoch\": 2.8, \"percentage\": 93.33, \"elapsed_time\": \"0:48:41\", \"remaining_time\": \"0:03:28\"}\n",
    "{\"current_steps\": 360, \"total_steps\": 375, \"loss\": 1.0788, \"lr\": 5.551554489528432e-07, \"epoch\": 2.88, \"percentage\": 96.0, \"elapsed_time\": \"0:50:07\", \"remaining_time\": \"0:02:05\"}\n",
    "{\"current_steps\": 370, \"total_steps\": 375, \"loss\": 1.0457, \"lr\": 7.8193242783281e-08, \"epoch\": 2.96, \"percentage\": 98.67, \"elapsed_time\": \"0:51:31\", \"remaining_time\": \"0:00:41\"}\n",
    "{\"current_steps\": 375, \"total_steps\": 375, \"epoch\": 3.0, \"percentage\": 100.0, \"elapsed_time\": \"0:52:12\", \"remaining_time\": \"0:00:00\"}\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f5679",
   "metadata": {},
   "source": [
    "![](/root/autodl-tmp/LLaMA-Factory/saves/qwen2.5-7b-instruct/lora/sft/training_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d951268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"models/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798b5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "lora_weights = \"/root/autodl-tmp/LLaMA-Factory/saves/qwen2.5-7b-instruct/lora/sft\"  # 替换为你的LoRA权重路径\n",
    "# 加载LoRA适配器并合并\n",
    "model = PeftModel.from_pretrained(model, lora_weights)\n",
    "model = model.merge_and_unload()  # 合并LoRA权重到基础模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1784af",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "小明家里有三只狗，他的哥哥送给他两只小狗，现在小明家里有几只狗？\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
